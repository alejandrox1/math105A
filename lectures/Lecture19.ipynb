{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence of iterative methods\n",
    "\n",
    "#### Theorem\n",
    "\n",
    "The fixed point method given by\n",
    "\n",
    "$$g(x) = Tx + b, \\quad x^{(k)} = g(x^{(k-1)}), \\quad x^{(0)} \\in \\mathbb R^{n}, k \\geq 1$$\n",
    "\n",
    "converges if $\\rho(T) < 1$ to the solution of $(I-T)x = b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Proof\n",
    "\n",
    "The iterates of the fixed-point method are\n",
    "\n",
    "$x^{(0)} = x^{(0)},$<br>\n",
    "$x^{(1)} = Tx^{(0)} + b,$<br>\n",
    "$x^{(2)} = T^2x^{(0)} + Tb + b,$<br>\n",
    "$x^{(3)} = T^3x^{(0)} + T^2b + T b + b,$<br>\n",
    "$\\vdots$<br>\n",
    "$x^{(k)} = T^k x^{(0)} + \\sum_{j=0}^{k-1} T^jb$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As $k \\to \\infty$, $\\|T^k\\| \\to 0$ and \n",
    "\n",
    "$$\\sum_{j=0}^{k-1} T^jb \\to (1-T)^{-1} b.$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$x = \\lim_{k \\to \\infty} x^{(k)} = (I-T)^{-1} b$$\n",
    "$$(I-T)x = b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Corollary (Error estimate)\n",
    "\n",
    "The fixed point method given by\n",
    "\n",
    "$$g(x) = Tx + c, \\quad x^{(k)}= g(x^{(k-1)}), \\quad x^{(0)} \\in \\mathbb R^{n}, k \\geq 1.$$\n",
    "\n",
    "with $\\rho(T) < 1$ has an error that satisfies\n",
    "\n",
    "$$\\|x^{(k)}- x\\| \\leq \\|T\\|^k \\|x^{(0)} - x\\|,\\quad x = (I-T)^{-1}b,$$\n",
    "\n",
    "in any norm.  Additionally, for any $\\delta > 0$\n",
    "\n",
    "$$ \\|x^{(k)}- x\\| = O( (\\rho(T) + \\delta)^n ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Proof\n",
    "\n",
    "From the proof of the previous theorem,\n",
    "\n",
    "$$x^{(k)} - x = T^k x^{(0)} + \\sum_{j=0}^{k-1} T^jb - \\sum_{j=0}^{\\infty}T^j b = T^k x^{(0)} - \\sum_{j=k}^{\\infty}T^j b = T^k\\left(  x^{(0)} - \\sum_{j=0}^{\\infty}T^j b \\right) = T^k (x^{(0)} -x).$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\|x^{(k)} - x \\| \\leq \\|T^k\\| \\|x^{(0)}-x\\| \\leq \\|T\\|^k \\|x^{(0)}-x\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The proof of the last fact requires the following two results that we will not prove:\n",
    "\n",
    "#### Theorem A\n",
    "\n",
    "For any $n\\times n$ matrix $A$ and $\\delta > 0$, there exists an induced matrix norm $\\|\\cdot\\|_A$ (depending on $A$) such that\n",
    "\n",
    "$$ \\|A\\|_A \\leq \\rho(A) + \\delta. $$\n",
    "\n",
    "#### Theorem B\n",
    "\n",
    "Given any two induced matrix norms on $n \\times n$ matrices $\\|\\cdot\\|_\\alpha$ and $\\|\\cdot\\|_\\beta$ there exists constants $0 < C_1 \\leq C_2$ such that\n",
    "\n",
    "$$C_1 \\|A\\|_\\beta \\leq \\|A\\|_\\alpha \\leq C_2 \\|A\\|_\\beta,$$\n",
    "\n",
    "for all matrices $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To finish the proof of the corollary, given $\\delta > 0$, we choose a matrix norm $\\|\\cdot\\|_T$ such that $ \\|T\\|_T \\leq \\rho(T) + \\delta$ by Theorem A.  Then from Theorem B we find some constant $C > 0$\n",
    "\n",
    "$$\\|x^{(k)} - x \\| \\leq \\|T^k\\| \\|x^{(0)}-x\\| \\leq C \\|T^k\\|_T \\|x^{(0)}-x\\| \\leq C \\|T\\|^k_T \\|x^{(0)}-x\\| \\leq C (\\rho(T) + \\delta)^k \\|x^{(0)}-x\\|. $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ \\|x^{(k)}- x\\| = O( (\\rho(T) + \\delta)^k ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jacobi's Method\n",
    "\n",
    "We want to solve the equation $Ax=b$.  If $A$ would happen to be a diagonal matrix, with non-zero diagonal entries, computing $A^{-1} = \\mathrm{diag}(a_{11}^{-1},a_{22}^{-1},\\ldots,a_{nn}^{-1})$ is simple.  But, of course, most linear systems are not diagonal.  The __Jacobi iterative method__ is given by the following iteration formula:\n",
    "\n",
    "\\begin{align*}\n",
    "x^{(0)} &= \\text{ an initial guess}\\\\\n",
    "x^{(k)}_{i} &= \\sum_{j=1, ~~ j\\neq i}^n \\left( - \\frac{a_{ij} x_j^{(k-1)}}{a_{ii}} \\right) + \\frac{b_i}{a_{ii}}, \\quad {i = 1,2,\\ldots,n}, ~~ k > 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "\n",
      "    0.0876\n",
      "    0.0786\n",
      "    0.1011\n",
      "    0.1082\n",
      "\n",
      "\n",
      "ans =\n",
      "\n",
      "   4.9674e-07\n"
     ]
    }
   ],
   "source": [
    "A = [10, -1 , 2, 0; -1, 11, -1 3; 2, -1, 10, -1; 0, 3,-1,8];\n",
    "b = [1,1,1,1]'; n = 4; x = zeros(n,1);\n",
    "x = Jacobi(A,b,.000001,40)\n",
    "norm(A*x-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Jacobi's method is easier to understand when written in matrix form, even though it should be coded as written above.  Let $A = D - L - U$ where $L$ is strictly lower-triangular, $D$ is diagonal and $U$ is strictly upper-triangular.  Then define\n",
    "\n",
    "$$ x^{(k)} = D^{-1}(L + U) x^{(k-1)} + D^{-1} b.$$\n",
    "\n",
    "You should check that this is the same as the above equations for Jacobi's method. A fixed-point of this iteration would satisfy\n",
    "\n",
    "$$ x = D^{-1}(L + U) x + D^{-1} b,$$\n",
    "$$ (D-L-U)x = b,$$\n",
    "$$ Ax = b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Theorem\n",
    "\n",
    "Jacobi's method converges if $\\rho(D^{-1}(L + U))< 1$.\n",
    "\n",
    "In particular, this may happen if $D$ has large entries (on the diagonal) and $L$ and $U$ have small entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gauss-Seidel Method\n",
    "\n",
    "The __Gauss-Seidel iterative method__ is a slight modification of Jacobi's method.  Recall Jacobi's method\n",
    "\n",
    "\\begin{align*}\n",
    "x^{(0)} &= \\text{ an initial guess}\\\\\n",
    "x^{(k)}_{i} &= \\sum_{j=1, ~~ j\\neq i}^n \\left( - \\frac{a_{ij} x_j^{(k-1)}}{a_{ii}} \\right) + \\frac{b_i}{a_{ii}}, \\quad {i = 1,2,\\ldots,n}, ~~ k > 0.\n",
    "\\end{align*}\n",
    "\n",
    "We can modify this by replacing the \"$k-1$\" on the right-hand side with $k$ when $j \\leq i$, since $x_j^{(k)}$ has been computed for $j < i$: \n",
    "\n",
    "\\begin{align*}\n",
    "x^{(0)} &= \\text{ an initial guess}\\\\\n",
    "x^{(k)}_{i} &=  -\\frac{1}{a_{ii}}\\left[\\sum_{j=1}^{i-1}  {a_{ij} x_j^{(k)}} + \\sum_{j=i+1}^n  {a_{ij} x_j^{(k-1)}} \\right] + \\frac{b_i}{a_{ii}}, \\quad {i = 1,2,\\ldots,n}, ~~ k > 0.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of interations reached, Nmax = 10\n",
      "ans =\n",
      "\n",
      "   8.2524e-05\n",
      "\n",
      "Maximum number of interations reached, Nmax = 10\n",
      "ans =\n",
      "\n",
      "   1.4308e-10\n"
     ]
    }
   ],
   "source": [
    "A = [10, -1 , 2, 0; -1, 11, -1 3; 2, -1, 10, -1; 0, 3,-1,8];\n",
    "b = [1,1,1,1]'; n = 4; x = zeros(n,1);\n",
    "x = Jacobi(A,b,0,10);\n",
    "norm(A*x-b)\n",
    "x = GS(A,b,0,10);\n",
    "norm(A*x-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $A = D - L - U$ where $L$ is strictly lower-triangular, $D$ is diagonal and $U$ is strictly upper-triangular.  Then define\n",
    "\n",
    "$$ x^{(k)} = (D-L)^{-1}U x^{(k-1)} + (D-L)^{-1} b.$$\n",
    "\n",
    "You should check that this gives the same iteration as the formula for Gauss-Seidel.  A fixed-point of this iteration would satisfy\n",
    "\n",
    "$$ x = (D-L)^{-1}U x +  (D-L)^{-1}b,$$\n",
    "$$ (D-L-U)x = b,$$\n",
    "$$ Ax = b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Theorem\n",
    "\n",
    "1. The Gauss-Seidel method converges if $\\rho((D-L)^{-1}U)< 1$.\n",
    "2. If the entries of $L$ and $U$ in $A = D-L-U$ are non-negative and $\\rho(D^{-1}(L +U)) < 1$ (Jacobi converges) then\n",
    "\n",
    "$$ \\rho((D-L)^{-1}U) < \\rho(D^{-1}(L +U)) \\quad \\text{(Gauss-Siedel converges faster)}$$ "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

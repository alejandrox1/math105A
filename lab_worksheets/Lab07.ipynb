{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will explore the convergence of iterative methods for linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Convergent iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we are going to study methods to solve $Ax = b$ that involve splitting $A$ as follows: $A = S - T$. Here let's choose $S=I$ and $T = I-A$. Then the solution to $Ax = b$ is the fixed point of $g(x) = Tx + b$. \n",
    "\n",
    "### Question \n",
    "\n",
    "Show this (on a sheet of paper). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Choose an initial guess $x^{(0)}$. Use fixed-point iteration to obtain an approximate solution to $Ax = b$, where $A$ and $b$ are: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "A = np.array([[1, .2, .3],\n",
    "              [-.1, 1.1, .3],\n",
    "              [.2, .5, 1]])\n",
    "b = np.array([[1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|Ax - b|: \n",
      "1.73205080757\n"
     ]
    }
   ],
   "source": [
    "def fixed_point_iteration(x0=np.array([[0],\n",
    "                                       [0],\n",
    "                                       [0]]), \n",
    "                          number_iterations=20):  \n",
    "    T = np.eye(3) - A\n",
    "    x = x0 \n",
    "    # your code here \n",
    "    \n",
    "    print \n",
    "    print '|Ax - b|: ' \n",
    "    print np.linalg.norm(np.dot(A,x) - b)\n",
    "\n",
    "fixed_point_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Divergent iteration\n",
    "\n",
    "Now consider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2.2, 0.],\n",
    "              [-3.1, 1.1, 0.],\n",
    "              [0., 0., .8]])\n",
    "b = np.array([[0.],\n",
    "              [0.],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Use `fixed_point_iteration` (together with its default parameters) to solve $Ax = b$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|Ax - b|: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "fixed_point_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Compute by hand (i.e. on a sheet of paper) $x_0$, $x_1$, $x_2$ and $x_3$. Can you explain why this method converges? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "As you did the hand calculations, you should have noticed that \n",
    "\n",
    "$$ x_k = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\sum_{n=0}^k (0.2)^n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "which tends to $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Show that the method does not converge with initial guess $x^{(0)} = [1,0,0]$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind the divergence of $x$ has to do with the eigenvalues of $T$. \n",
    "\n",
    "### Question \n",
    "\n",
    "Use `LA.eigvals` from the numpy library to find the eigenvalues of $T$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues of T:\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "I = np.eye(3)\n",
    "T = None # your code here \n",
    "print 'eigenvalues of T:', # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case two are complex (Python uses `j` to represent the imaginary number $\\sqrt{-1}$) and one is real. Clearly, the largest eigenvalue of $T$ has magnitude greater than unity, i.e. $\\rho(T) > 1$. It turns out that $T$-matrices with $\\rho(T) < 1$ always converge but no guarantee can be made when $\\rho(T) > 1$. No surprise then that we could find starting values of $x$ that lead to a divergent sequence $\\{x_k\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Fixing a divergent iteration scheme\n",
    "\n",
    "Now consider the matrix $A$ and the column vector $b$: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, .2, 0.],\n",
    "              [-.3, 4, 1.],\n",
    "              [0., 1., 8]])\n",
    "b = np.array([[1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Use `fixed_point_iteration` to show that fixed-point iteration does not converge. \n",
    "\n",
    "### Solution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Preconditioning](https://en.wikipedia.org/wiki/Preconditioner)\n",
    "\n",
    "Solving $Ax = b$ is equivalent to solving $PAx = Pb$. Suppose however that $PA \\approx I$. Then $T = I - PA \\approx 0$. We therefore expect that the eigenvalues of $T$ are distributed around zero. This is great news because it opens up the possibility that the spectral radius of $T$ is smaller than unity, in which case fixed-point iteration should converge. \n",
    "\n",
    "One strategy for coming up with $P$ is to scale each row of $A$ by its diagonal element. This makes each diagonal element equal to unity, as in the identity matrix. Moreover, if the off-diagonal elements of $A$ in any given row are smaller than the diagonal element in that row, then the off-diagonal elements of the scaled matrix are expected to be smaller than unity, hopefully much smaller, just as they are in the identity matrix. This is expected to be a particularly common occurrence if the original matrix is sparse. \n",
    "\n",
    "One way to scale each row by its diagonal element is to construct a diagonal matrix $D$ containing the diagonal elements of $A$ and compute $D^{-1}A$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: \n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  4.  0.]\n",
      " [ 0.  0.  8.]]\n",
      "I_approx: \n",
      "[[ 1.     0.2    0.   ]\n",
      " [-0.075  1.     0.25 ]\n",
      " [ 0.     0.125  1.   ]]\n",
      "eigenvalues of T = I - A: [-0.02115992 -2.74320906 -7.23563102]\n",
      "eigenvalues of T = I - I_approx: [ -1.27475488e-01   2.51534904e-17   1.27475488e-01]\n"
     ]
    }
   ],
   "source": [
    "D = np.diagflat(np.diag(A))\n",
    "print 'D: '\n",
    "print D \n",
    "\n",
    "I_approx = np.dot(np.linalg.inv(D), A)\n",
    "print 'I_approx: '\n",
    "print I_approx\n",
    "\n",
    "print 'eigenvalues of T = I - A:', LA.eigvals(I - A)\n",
    "print 'eigenvalues of T = I - I_approx:', LA.eigvals(I - I_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the original system $Ax = b$, we have $\\rho(T) > 1$, whereas in the preconditioned system $D^{-1}A = D^{-1}b$, we have $\\rho(T) < 1$. \n",
    "\n",
    "### Question\n",
    "\n",
    "Use fixed-point iteration to solve the pre-conditioned system. \n",
    "\n",
    "### Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|Ax - b|: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here \n",
    "\n",
    "print \n",
    "print '|Ax - b|: ' \n",
    "print # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

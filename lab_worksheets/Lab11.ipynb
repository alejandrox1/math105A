{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD and time series \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates an $m \\times n$ matrix that represents the evolution of $n$ quantities over the course of $m$ days, e.g. stocks, temperatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def generate_data(m, n):\n",
    "    T = np.zeros((m,m))\n",
    "    T[0,0] = 1.001\n",
    "    T[1,0] = -1.001\n",
    "    T[2,0] = .01\n",
    "    T[2,2] = 1.01\n",
    "\n",
    "    F = 0.1*np.ones(m)\n",
    "    F[0] = 0.5\n",
    "    F[1] = 0.5\n",
    "    F[2] = 0.0\n",
    "\n",
    "    np.random.seed(42)    \n",
    "\n",
    "    Xt = np.random.uniform(size=(m,n))\n",
    "    Xt[:,0] = np.random.normal(size=m)\n",
    "    Xt[0,0] = .1\n",
    "    Xt[1,0] = 1\n",
    "    Xt[2] = .01\n",
    "    \n",
    "    for i in range(1,n):\n",
    "        # this builds up the time correlations:\n",
    "        Xt[:,i] = np.dot(T,Xt[:,i-1]) + F*np.random.normal(size=m) \n",
    "    \n",
    "    return Xt.T # transpose so that rows represent time \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "X = generate_data(10,250) # 10 quantities tracked over 250 days\n",
    "_ = plt.plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Plot the time series for the first two quantities only. \n",
    "\n",
    "### Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that if one goes up, the other generally goes down. \n",
    "\n",
    "### Problem \n",
    "\n",
    "How would you detect this anti correlation? One approach is to ask if one trajectory is just the negative of the other. Compute the norm of `X[:,0] + X[:,0]`. Deduce that one trajectory is not just the negative of the other.\n",
    "\n",
    "### Solution \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'norm of sum of first two trajs: '\n",
    "print None # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So based on this metric, we wouldn't conclude that there is an anti-correlation. But what if the anti-correlation is polluted by noise ? These are noisy trajectories after all. \n",
    "\n",
    "SVD can address this problem by filtering out the noise! The basic idea is to compute the directions of maximum variability in the data and project the data onto the vector subspace defined by those directions. The remaining directions, which capture the fine-scale variability of the data, are thus filtered out. \n",
    "\n",
    "\n",
    "### Problem \n",
    "\n",
    "Fill in the following code that performs a SVD of the data and computes `X_approx`, which projects the data onto the first $k$ principal eigenvectors. What is the norm of `X_approx[:,0] + X_approx[:,0]`? Interpret your result. \n",
    "\n",
    "### Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg \n",
    "\n",
    "def create_Sigma(U,s,Vt): \n",
    "    m = len(U)\n",
    "    n = len(Vt)\n",
    "    Sigma = np.zeros((m, n))\n",
    "    for i in range(min(m, n)):\n",
    "        Sigma[i, i] = s[i]\n",
    "    return Sigma\n",
    "    \n",
    "def compare_exact_to_approximation(trajectories, s_cutoff):           \n",
    "    # SVD: compute U, S, Vt \n",
    "    # your code here \n",
    "    \n",
    "    # project data onto s_cutoff principal eigenvectors\n",
    "    # your code here \n",
    "    \n",
    "    # compare exact and approximated trajectories\n",
    "    plt.plot(X[:,trajectories])\n",
    "    plt.plot(X_approx[:,trajectories])\n",
    "    \n",
    "    return X_approx\n",
    "\n",
    "X_approx = compare_exact_to_approximation(trajectories=[0,1], s_cutoff=1)\n",
    "\n",
    "print 'norm of sum of approximations of first two trajs: '\n",
    "print np.linalg.norm(X_approx[:,0] + X_approx[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that the approximate trajectories are a good deal more anti-correlated than the exact trajectories. Thus approximating time courses via SVD filters out noise and allows us to spot correlations more easily. \n",
    "\n",
    "Trajectories 0 and 1 represent components of data vectors. These components don't shift very much when the data is projected onto the vector subspace spanned by the first eigenvector. (This is what `compare_exact_to_approximation` does when `s_cutoff=1`.)  For these components, the first-order approximation is good. \n",
    "\n",
    "Now look at trajectory 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compare_exact_to_approximation(trajectories=2, s_cutoff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory 2 represents a component that does shift alot when data are projected onto the vector subspace spanned by the first principal eigenvector. In other words the approximation is poor for such data. \n",
    "\n",
    "### Problem \n",
    "\n",
    "Remedy the poor approximation of trajectory 2 by projecting the data onto the vector subspace spanned by the first two principal eigenvectors. \n",
    "\n",
    "### Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

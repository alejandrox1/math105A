{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will explore the convergence of iterative methods for linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Convergent iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we are going to study methods to solve $Ax = b$ that involve splitting $A$ as follows: $A = S - T$. Here let's choose $S=I$ and $T = I-A$. Then the solution to $Ax = b$ is the fixed point of $g(x) = Tx + b$. \n",
    "\n",
    "### Question \n",
    "\n",
    "Show this (on a sheet of paper). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Choose an initial guess $x^{(0)}$. Use fixed-point iteration to obtain an approximate solution to $Ax = b$, where $A$ and $b$ are: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "A = np.array([[1, .2, .3],\n",
    "              [-.1, 1.1, .3],\n",
    "              [.2, .5, 1]])\n",
    "b = np.array([[1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0 0 0]]\n",
      "x =  [[ 1.  1.  1.]]\n",
      "x =  [[ 0.5  0.7  0.3]]\n",
      "x =  [[ 0.77  0.89  0.55]]\n",
      "x =  [[ 0.657  0.823  0.401]]\n",
      "x =  [[ 0.7151  0.8631  0.4571]]\n",
      "x =  [[ 0.69025  0.84807  0.42543]]\n",
      "x =  [[ 0.702757  0.856589  0.437915]]\n",
      "x =  [[ 0.6973077  0.8532423  0.4311541]]\n",
      "x =  [[ 0.70000531  0.85506031  0.43391731]]\n",
      "x =  [[ 0.69881275  0.85431931  0.43246878]]\n",
      "x =  [[ 0.6993955   0.85470871  0.4330778 ]]\n",
      "x =  [[ 0.69913492  0.85454534  0.43276654]]\n",
      "x =  [[ 0.69926097  0.85462899  0.43290035]]\n",
      "x =  [[ 0.6992041   0.85459309  0.43283331]]\n",
      "x =  [[ 0.69923139  0.85461111  0.43286263]]\n",
      "x =  [[ 0.69921899  0.85460324  0.43284817]]\n",
      "x =  [[ 0.6992249   0.85460712  0.43285458]]\n",
      "x =  [[ 0.6992222   0.8546054   0.43285146]]\n",
      "x =  [[ 0.69922348  0.85460624  0.43285286]]\n",
      "\n",
      "|Ax - b|: \n",
      "4.51478903588e-07\n"
     ]
    }
   ],
   "source": [
    "def fixed_point_iteration(x0=np.array([[0],\n",
    "                                       [0],\n",
    "                                       [0]]), \n",
    "                          number_iterations=20):  \n",
    "    T = np.eye(3) - A\n",
    "    x = x0\n",
    "    for i in range(number_iterations):\n",
    "        print 'x = ', x.T\n",
    "        x = np.dot(T,x) + b\n",
    "\n",
    "    print \n",
    "    print '|Ax - b|: ' \n",
    "    print np.linalg.norm(np.dot(A,x) - b)\n",
    "\n",
    "fixed_point_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Divergent iteration\n",
    "\n",
    "Now consider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2.2, 0.],\n",
    "              [-3.1, 1.1, 0.],\n",
    "              [0., 0., .8]])\n",
    "b = np.array([[0.],\n",
    "              [0.],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Use `fixed_point_iteration` (together with its default parameters) to solve $Ax = b$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0 0 0]]\n",
      "x =  [[ 0.  0.  1.]]\n",
      "x =  [[ 0.   0.   1.2]]\n",
      "x =  [[ 0.    0.    1.24]]\n",
      "x =  [[ 0.     0.     1.248]]\n",
      "x =  [[ 0.      0.      1.2496]]\n",
      "x =  [[ 0.       0.       1.24992]]\n",
      "x =  [[ 0.        0.        1.249984]]\n",
      "x =  [[ 0.         0.         1.2499968]]\n",
      "x =  [[ 0.          0.          1.24999936]]\n",
      "x =  [[ 0.          0.          1.24999987]]\n",
      "x =  [[ 0.          0.          1.24999997]]\n",
      "x =  [[ 0.          0.          1.24999999]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "x =  [[ 0.    0.    1.25]]\n",
      "\n",
      "|Ax - b|: \n",
      "1.06581410364e-14\n"
     ]
    }
   ],
   "source": [
    "fixed_point_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Compute by hand (i.e. on a sheet of paper) $x_0$, $x_1$, $x_2$ and $x_3$. Can you explain why this method converges? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "As you did the hand calculations, you should have noticed that \n",
    "\n",
    "$$ x_k = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\sum_{n=0}^k (0.2)^n\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "which tends to \n",
    "\n",
    "$$ x = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "1.25\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "as $k\\rightarrow\\infty$. But 0.2 is an eigenvalue of the $T$ matrix because 0.2 sits in a column and a row all by itself. Thus the convergence of the method hinges on the fact that one of $T$'s eigenvalues is less than unity. This is no accident! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Show that the method does not converge with initial guess $x^{(0)} = [1,0,0]$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[1 0 0]]\n",
      "x =  [[ 0.   3.1  1. ]]\n",
      "x =  [[-6.82 -0.31  1.2 ]]\n",
      "x =  [[  0.682 -21.111   1.24 ]]\n",
      "x =  [[ 46.4442   4.2253   1.248 ]]\n",
      "x =  [[  -9.29566  143.55449    1.2496 ]]\n",
      "x =  [[-315.819878  -43.171995    1.24992 ]]\n",
      "x =  [[  94.978389  -974.7244223    1.249984 ]]\n",
      "x =  [[  2.14439373e+03   3.91905448e+02   1.24999680e+00]]\n",
      "x =  [[ -8.62191986e+02   6.60843002e+03   1.24999936e+00]]\n",
      "x =  [[ -1.45385460e+04  -3.33363816e+03   1.24999987e+00]]\n",
      "x =  [[  7.33400395e+03  -4.47361289e+04   1.24999997e+00]]\n",
      "x =  [[  9.84194836e+04   2.72090251e+04   1.24999999e+00]]\n",
      "x =  [[ -5.98598553e+04   3.02379497e+05   1.25000000e+00]]\n",
      "x =  [[ -6.65234892e+05  -2.15803501e+05   1.25000000e+00]]\n",
      "x =  [[  4.74767702e+05  -2.04064782e+06   1.25000000e+00]]\n",
      "x =  [[  4.48942520e+06   1.67584466e+06   1.25000000e+00]]\n",
      "x =  [[ -3.68685825e+06   1.37496336e+07   1.25000000e+00]]\n",
      "x =  [[ -3.02491940e+07  -1.28042239e+07   1.25000000e+00]]\n",
      "x =  [[  2.81692927e+07  -9.24920790e+07   1.25000000e+00]]\n",
      "\n",
      "|Ax - b|: \n",
      "669461396.795\n"
     ]
    }
   ],
   "source": [
    "fixed_point_iteration(x0=np.array([[1],\n",
    "                                   [0],\n",
    "                                   [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind the divergence of $x$ has to do with the eigenvalues of $T$. \n",
    "\n",
    "### Question \n",
    "\n",
    "Use `LA.eigvals` from the numpy library to find the eigenvalues of $T$. \n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues of T: [-0.05+2.61103428j -0.05-2.61103428j  0.20+0.j        ]\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "I = np.eye(3)\n",
    "T = I - A\n",
    "print 'eigenvalues of T:', LA.eigvals(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case two are complex (Python uses `j` to represent the imaginary number $\\sqrt{-1}$) and one is real. Clearly, the largest eigenvalue of $T$ has magnitude greater than unity, i.e. $\\rho(T) > 1$. It turns out that $T$-matrices with $\\rho(T) < 1$ always converge but no guarantee can be made when $\\rho(T) > 1$. No surprise then that we could find starting values of $x$ that lead to a divergent sequence $\\{x_k\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Fixing a divergent iteration scheme\n",
    "\n",
    "Now consider the matrix $A$ and the column vector $b$: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, .2, 0.],\n",
    "              [-.3, 4, 1.],\n",
    "              [0., 1., 8]])\n",
    "b = np.array([[1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Use `fixed_point_iteration` to show that fixed-point iteration does not converge. \n",
    "\n",
    "### Solution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0 0 0]]\n",
      "x =  [[ 1.  1.  1.]]\n",
      "x =  [[ 0.8 -2.7 -7. ]]\n",
      "x =  [[  1.54  16.34  52.7 ]]\n",
      "x =  [[  -2.268 -100.258 -384.24 ]]\n",
      "x =  [[   21.0516   685.3336  2790.938 ]]\n",
      "x =  [[  -136.06672  -4839.62332 -20220.8996 ]]\n",
      "x =  [[    968.924664   34699.949544  146386.92052 ]]\n",
      "x =  [[   -6938.9899088  -250195.0917528 -1059407.393184 ]]\n",
      "x =  [[   50040.01835056  1807911.97146976  7666047.8440408 ]]\n",
      "x =  [[  -361581.39429395 -13074770.75294491 -55470245.87975536]]\n",
      "x =  [[  2.61495515e+06   9.45860847e+07   4.01366493e+08]]\n",
      "x =  [[ -1.89172159e+07  -6.84340260e+08  -2.90415153e+09]]\n",
      "x =  [[  1.36868053e+08   4.95149715e+09   2.10134010e+10]]\n",
      "x =  [[ -9.90299429e+08  -3.58268320e+10  -1.52045304e+11]]\n",
      "x =  [[  7.16536641e+09   2.59228710e+11   1.10014396e+12]]\n",
      "x =  [[ -5.18457421e+10  -1.87568048e+12  -7.96023644e+12]]\n",
      "x =  [[  3.75136096e+11   1.35717242e+13   5.75973355e+13]]\n",
      "x =  [[ -2.71434483e+12  -9.81999672e+13  -4.16753073e+14]]\n",
      "x =  [[  1.96399934e+13   7.10538671e+14   3.01547148e+15]]\n",
      "\n",
      "|Ax - b|: \n",
      "1.84616658789e+17\n"
     ]
    }
   ],
   "source": [
    "fixed_point_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Preconditioning](https://en.wikipedia.org/wiki/Preconditioner)\n",
    "\n",
    "Solving $Ax = b$ is equivalent to solving $PAx = Pb$. Suppose however that $PA \\approx I$. Then $T = I - PA \\approx 0$. We therefore expect that the eigenvalues of $T$ are distributed around zero. This is great news because it opens up the possibility that the spectral radius of $T$ is smaller than unity, in which case fixed-point iteration should converge. \n",
    "\n",
    "One strategy for coming up with $P$ is to scale each row of $A$ by its diagonal element. This makes each diagonal element equal to unity, as in the identity matrix. Moreover, if the off-diagonal elements of $A$ in any given row are smaller than the diagonal element in that row, then the off-diagonal elements of the scaled matrix are expected to be smaller than unity, hopefully much smaller, just as they are in the identity matrix. This is expected to be a particularly common occurrence if the original matrix is sparse. \n",
    "\n",
    "One way to scale each row by its diagonal element is to construct a diagonal matrix $D$ containing the diagonal elements of $A$ and compute $D^{-1}A$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: \n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  4.  0.]\n",
      " [ 0.  0.  8.]]\n",
      "I_approx: \n",
      "[[ 1.     0.2    0.   ]\n",
      " [-0.075  1.     0.25 ]\n",
      " [ 0.     0.125  1.   ]]\n",
      "eigenvalues of T = I - A: [-0.02115992 -2.74320906 -7.23563102]\n",
      "eigenvalues of T = I - I_approx: [ -1.27475488e-01   2.51534904e-17   1.27475488e-01]\n"
     ]
    }
   ],
   "source": [
    "D = np.diagflat(np.diag(A))\n",
    "print 'D: '\n",
    "print D \n",
    "\n",
    "I_approx = np.dot(np.linalg.inv(D), A)\n",
    "print 'I_approx: '\n",
    "print I_approx\n",
    "\n",
    "print 'eigenvalues of T = I - A:', LA.eigvals(I - A)\n",
    "print 'eigenvalues of T = I - I_approx:', LA.eigvals(I - I_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the original system $Ax = b$, we have $\\rho(T) > 1$, whereas in the preconditioned system $D^{-1}A = D^{-1}b$, we have $\\rho(T) < 1$. \n",
    "\n",
    "### Question\n",
    "\n",
    "Use fixed-point iteration to solve the pre-conditioned system. \n",
    "\n",
    "### Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0 0 0]]\n",
      "x =  [[ 1.     0.25   0.125]]\n",
      "x =  [[ 0.95     0.29375  0.09375]]\n",
      "x =  [[ 0.94125     0.2978125   0.08828125]]\n",
      "x =  [[ 0.9404375   0.29852344  0.08777344]]\n",
      "x =  [[ 0.94029531  0.29858945  0.08768457]]\n",
      "x =  [[ 0.94028211  0.29860101  0.08767632]]\n",
      "x =  [[ 0.9402798   0.29860208  0.08767487]]\n",
      "x =  [[ 0.94027958  0.29860227  0.08767474]]\n",
      "x =  [[ 0.94027955  0.29860228  0.08767472]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "x =  [[ 0.94027954  0.29860229  0.08767471]]\n",
      "\n",
      "|Ax - b|: \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "T = I - I_approx\n",
    "x = np.array([[0],[0],[0]])\n",
    "for i in range(20):\n",
    "    print 'x = ', x.T\n",
    "    x = np.dot(T,x) + np.dot(np.linalg.inv(D), b)\n",
    "\n",
    "print \n",
    "print '|Ax - b|: ' \n",
    "print np.linalg.norm(np.dot(A,x) - b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
